```{python}
import pandas as pd
import re
import nltk
import numpy as np
import os
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
# from nltk.corpus import symbols
from string import punctuation
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report as clf_report

```

```{python}

def BoM_verse_extract(file_path):
    speakers_with_verse = {}
    authors_with_verse = {}
    # Open the file in read mode
    with open(file_path, "r", encoding="latin-1") as file:
        # Iterate over each line in the file
        iterator = 0
        for line in file:
            iterator += 1
            # Process each line as needed
            line = line.replace("(", "( ")
            line = line.replace("\t", " ")
            line = line.replace("\n`", " ")
            line = line.replace("\xa0", " ")
            line = line.strip()
            line_split = line.split(" ")
            if line_split[0] == "(" or len(line_split) == 1:
                for i in range(len(line_split)):
                    if len(line_split) == 1:
                        name = line
                        break
                    elif line_split[i] == ")":
                        name = line_split[i + 1:]
                        name = " ".join(name).lower()
                        break
            elif  ":" in line:
                if name not in speakers_with_verse:
                        speakers_with_verse[name] = []
                if "THE BOOK OF" not in line:
                    speakers_with_verse[name].append((line, line))
            if line_split[0] == "(" or len(line_split) == 1:
                for i in range(len(line_split)):
                    if len(line_split) == 1:
                        name = line
                        name = name.lower()
                        if name == "":
                            print("")
                        break
                    elif line_split[i] == ")":
                        name = line_split[1]
                        name = name.lower()
                        if name == "":
                            print("")
                        break
            elif ":" in line:
                if name not in authors_with_verse:
                        authors_with_verse[name] = []
                if "THE BOOK OF" not in line:
                    authors_with_verse[name].append((line, line))
    return authors_with_verse
def BoM_verse_seperation(file_path, count):
    speakers_with_verse = {}
    authors_with_verse = {}
    # Open the file in read mode
    with open(file_path, "r", encoding="latin-1") as file:
        # Iterate over each line in the file
        iterator = 0
        for line in file:
            iterator += 1
            # Process each line as needed
            line = line.replace("(", "( ")
            line = line.replace("\t", " ")
            line = line.replace("\n`", " ")
            line = line.replace("\xa0", " ")
            line = line.strip()
            line_split = line.split(" ")
            if line_split[0] == "(" or len(line_split) == 1:
                for i in range(len(line_split)):
                    if len(line_split) == 1:
                        name = line
                        break
                    elif line_split[i] == ")":
                        name = line_split[i + 1:]
                        name = " ".join(name).lower()
                        break
            elif  ":" in line:
                if name not in speakers_with_verse:
                        speakers_with_verse[name] = []
                if "THE BOOK OF" not in line:
                    speakers_with_verse[name].append((line, line))
            if line_split[0] == "(" or len(line_split) == 1:
                for i in range(len(line_split)):
                    if len(line_split) == 1:
                        name = line
                        name = name.lower()
                        if name == "":
                            print("")
                        break
                    elif line_split[i] == ")":
                        name = line_split[1]
                        name = name.lower()
                        if name == "":
                            print("")
                        break
            elif ":" in line:
                if name not in authors_with_verse:
                        authors_with_verse[name] = []
                if "THE BOOK OF" not in line:
                    authors_with_verse[name].append((line, line))

    max_list_length = max(len(lst) for lst in authors_with_verse.values())
    num_divisions = count  # Divide each list into 'count' parts

    # Initialize the list of dictionaries for divided entries
    divided_dicts = [{} for _ in range(num_divisions)]

    # Iterate over the original dictionary
    for key, values_list in authors_with_verse.items():
        # Calculate the chunk size for each list
        chunk_size = len(values_list) // num_divisions

        # Divide each list into chunks and assign to the corresponding dictionary
        for i in range(num_divisions):
            start_idx = i * chunk_size
            end_idx = (i + 1) * chunk_size if i < num_divisions - 1 else len(values_list)
            divided_dicts[i][key] = values_list[start_idx:end_idx]

    return divided_dicts
```

```{python}
count = 0
stop_words = set(stopwords.words('english'))
symbols = set(punctuation)


speakers_word_freq = {}
authors_word_freq = {}
authors_function_word_freq = {}
```
```{python}
function_words_king_james = [
    'the', 'and', 'of', 'to', 'in', 'that', 'which', 'he', 'unto', 'for', 
    'is', 'shall', 'not', 'it', 'be', 'his', 'with', 'they', 'him', 'as', 
    'all', 'thou', 'thy', 'was', 'from', 'they', 'them', 'their', 'have', 
    'will', 'are', 'or', 'your', 'our', 'we', 'ye', 'if', 'my', 'me', 
    'upon', 'were', 'by', 'do', 'but', 'what', 'an', 'there', 'then', 
    'also', 'into', 'when', 'had', 'hath', 'been', 'should', 'now', 
    'one', 'more', 'did', 'himself', 'her', 'out', 'some', 'these', 
    'other', 'said', 'no', 'therefore', 'like', 'how', 'man', 'who', 
    'yea', 'ye', 'such', 'didst', 'shalt', 'thyself', 'whom', 'may', 
    'again', 'than', 'every', 'must', 'without', 'where', 'himself', 
    'much', 'their', 'thine', 'those', 'let', 'would', 'say', 'wherefore', 
    'been', 'whosoever', 'could', 'make', 'being', 'whence', 'thus', 
    'shouldst', 'unto', 'were', 'wherein', 'therein', 'yet', 'till', 
    'among', 'according', 'unto', 'many', 'among', 'also', 'whither', 
    'because', 'whose', 'neither', 'whoso', 'whosoever', 'forth', 
    'moreover', 'forth', 'may', 'hereof', 'each', 'whereof', 'whosoever', 
    'once', 'ever', 'wherewith', 'ought', 'hence', 'forth', 'wherewithal', 
    'shouldest', 'hast', 'hast', 'seemeth', 'became', 'thereby', 'anything', 
    'whoso', 'wherewithal', 'whither', 'unto', 'throughout', 'lest', 'thee', 
    'whereto', 'wherewith', 'wheresoever', 'seest', 'seest', 'thy', 'wheresoever', 
    'oft', 'cometh', 'whosoever', 'because', 'doeth', 'whosoever', 'forth', 
    'becometh', 'soever', 'oftentimes', 'wherefore', 'notwithstanding', 
    'shouldest', 'seem', 'verily', 'whereto', 'wherewithal', 'hast', 
    'underneath', 'upwards', 'whatsoever', 'wherewith', 'notwithstanding', 
    'whensoever', 'whither', 'seest', 'because', 'upwards', 'hast', 'doeth', 
    'whither', 'seest', 'abundance', 'doeth', 'whosoever', 'upwards', 'whence', 
    'whereunto', 'thereunto', 'henceforth', 'forthwith', 'throughout', 
    'seest', 'whereto', 'thereof', 'whatsoever', 'wherefore', 'becometh', 
    'whither', 'forthwith', 'wherewith', 'whatsoever', 'whosoever', 'whosoever', 
    'whereby', 'upward', 'therein', 'thee', 'whosoever', 'because', 'howsoever', 
    'forthwith', 'thine', 'whereon', 'wherein', 'whoso', 'whereto', 'wherewithal', 
    'wheresoever', 'downward', 'wherewith', 'outward', 'notwithstanding', 
    'thereunto', 'thence', 'thine', 'wherewith', 'whosoever', 'thereon', 
    'whatsoever', 'notwithstanding', 'wherewithal', 'forthwith', 'notwithstanding', 
    'doeth', 'upward', 'thereof', 'becometh', 'soever', 'therefrom', 'whosoever', 
    'whoso', 'inasmuch', 'whereon', 'whosoever', 'whereby', 'therewith', 'outward', 
    'forthwith', 'whither', 'seest', 'soever', 'whereto', 'thine', 'doeth', 
    'abundance', 'doeth', 'soever', 'whereby', 'forthwith', 'seest', 'whereto', 
    'thereupon', 'whereunto', 'thereupon', 'whereon', 'forthwith', 'wherewithal', 
    'whatsoever', 'soever', 'soever', 'wherefore', 'forthwith', 'therein', 'henceforth', 
    'abundance', 'wherewithal', 'seest', 'wherewithal', 'upward', 'wherein', 
    'wherewithal' ]

def filter_function_words(tokenized_string, function_words_list):
    filtered_tokens = [token for token in tokenized_string if token in function_words_list]
    return filtered_tokens
```

```{python}
for author, verses in speakers_with_verse.items():
    # Combine all verses into a single string for the author
        author_text = " ".join(verse[1] for verse in verses)
        
        # Tokenize the text
        tokens = word_tokenize(author_text)
        
        # Lowercase all tokens for case-insensitive analysis
        tokens = [token.lower() for token in tokens if token.lower() not in symbols]
        
        # Filter out stop words and symbols
        tokens = [token for token in tokens if token not in stop_words]
        
        # Count the frequency of each word
        word_freq = Counter(tokens)

        word_freq = dict(word_freq)
        
        # Store word frequencies for the author
        speakers_word_freq[author] = word_freq
```
```{python}

for author, verses in authors_with_verse.items():
    # Combine all verses into a single string for the author
        author_text = " ".join(verse[1] for verse in verses)
        
        # Tokenize the text
        tokens = word_tokenize(author_text)
        
        # Lowercase all tokens for case-insensitive analysis
        tokens = [token.lower() for token in tokens if token.lower() not in symbols]
        
        # # Filter out stop words and symbols
        tokens = [token for token in tokens if token not in stop_words]
        
        # Count the frequency of each word
        word_freq = Counter(tokens)

        word_freq = dict(word_freq)
        
        # Store word frequencies for the author
        authors_word_freq[author] = word_freq

```

```{python}
def tokenize_dict(text_list):
    tokenized_text_list = []
    authors_function_word_freq = {}
    for i in text_list:      
        authors_function_word_freq = {}
        for author, verses in i.items():

            # Combine all verses into a single string for the author
                author_text = " ".join(verse[1] for verse in verses)
                
                # Tokenize the text
                tokens = word_tokenize(author_text)
                
                # Lowercase all tokens for case-insensitive analysis
                tokens = [token.lower() for token in tokens if token.lower() not in symbols]
                
                tokens = filter_function_words(tokens, function_words_king_james)
                
                # Count the frequency of each word
                word_freq = Counter(tokens)

                word_freq = dict(word_freq)
                
                # Store word frequencies for the author
                authors_function_word_freq[author] = word_freq
        tokenized_text_list.append(authors_function_word_freq)
    return tokenized_text_list
#tokenized_text_list = tokenize_dict(BoM_verse_seperation("BoM.txt", 10))
```
```{python}
def split_author_dict(dict_full, split_count):
    data = dict_full
    print(data)
    split_data = {f"{author}_{i}": {} for author in data for i in range(1,split_count + 1)}


    # Initialize lists to track the words for each section
    section_words = {}
    word_list = []
    # Iterate over each author
    for author, words_dict in data.items():
        for word, count in words_dict.items():
            if word not in word_list:
                word_list.append(word)
            mod_count = -1
            for split_author, split_words_dict in split_data.items():
                
                if author in split_author:
                    if mod_count == -1:
                        mod_count = count % split_count
                    
                    new_count = count // split_count
                    if mod_count > 0:
                        new_count += 1
                        mod_count -= 1
                    else:
                        pass
                    split_data[split_author][word] = new_count

    return(split_data)

    
```
```{python}

for author, word_freq in speakers_word_freq.items():
    print(f"\nWord frequencies for {author}:")
    for word, freq in word_freq.most_common(5):
        print(f"{word}: {freq}")
```
```{python}
for author, word_freq in authors_word_freq.items():
    print(f"\nWord frequencies for {author}:")
    for word, freq in word_freq.most_common(5):
        print(f"{word}: {freq}")
```

```{python}
authors_word_variance = {}
speakers_word_variance = {}
```

```{python}
# Iterate over each author's word frequencies
for author, word_freq in speakers_word_freq.items():
    # Extract word frequencies
    word_frequencies = np.array(list(word_freq.values()))
    
    # Calculate variance of word frequencies
    variance = np.var(word_frequencies)
    
    # Store word variance for the author
    speakers_word_variance[author] = variance
```
```{python}

# Iterate over each author's word frequencies
for author, word_freq in authors_word_freq.items():
    # Extract word frequencies
    word_frequencies = np.array(list(word_freq.values()))
    
    # Calculate variance of word frequencies
    variance = np.var(word_frequencies)
    
    # Store word variance for the author
    authors_word_variance[author] = variance

```
```{python}
total = 0
average_count = 0
for key, value in speakers_word_freq.items():
     average_count += 1
     total += len(value)
average = total / average_count
print(average)
note_worthy_speakers = []
# for key, value in speakers_word_freq.items():
#      note_worthy_speakers.append(key) if len(value) > 250 else None
# for author, variance in authors_word_variance.items():
#     if author in note_worthy_speakers:
#         speakers_word_freq[author]
#         print(f"Word variance for {author}: {variance}, word count: {len(speakers_word_freq[author])} ")
```
```{python}
for author, variance in authors_word_variance.items():
        print(f"Word variance for {author}: {variance}, word count: {len(authors_word_freq[author])} ")
```

```{python}
import csv
def author_dict_to_csv(data, filename, word_list):
    # Prepare data for transposing
    all_words = word_list
    author_data = defaultdict(dict)
    for author_section, word_counts in data.items():
        author = author_section.split('_')[0]
        for word, count in word_counts.items():
            all_words.add(word)
            author_data[author][word] = count

    # Transpose data for writing to CSV
    rows = [{'Author': 'Author'}]  # Initialize the first row with 'Author'
    rows[0].update({word: word for word in all_words})  # Add words to the first row
    for author, word_counts in author_data.items():
        row = {'Author': author}
        row.update({word: word_counts.get(word, 0) for word in all_words})
        rows.append(row)

    # Write data to CSV file
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['Author'] + list(all_words)
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)

def passage_dict_to_csv(data, filename):
    # Flatten the dictionary into a list of dictionaries
    rows = []
    for word, word_counts in data.items():
            if any(letter.isalpha() for letter in word):
                rows.append({'Word': word, 'Count': word_counts})

    # Write data to CSV file
    with open(filename, 'w', newline='', encoding='latin-1') as csvfile:
        fieldnames = ['Word', 'Count']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)

def named_author_dict_to_csv(author, data, filename):
    # Flatten the dictionary into a list of dictionaries
    rows = []
    for word, word_counts in data.items():
            if any(letter.isalpha() for letter in word):
                rows.append({'Author' : author, 'Word': word, 'Count': word_counts})

    # Write data to CSV file
    with open(filename, 'w', newline='', encoding='latin-1') as csvfile:
        fieldnames = ['Author', 'Word', 'Count']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)


# author_dict_to_csv(authors_function_word_freq, "authors_only_function.csv")
```
```{python}
import csv
import os
from collections import defaultdict
def create_section_csv(split_author_dict, count, iterator):
    # Create the 'authors_csv' folder if it doesn't exist
    if not os.path.exists('authors_csv'):
        os.makedirs('authors_csv')
    split_data = split_author_dict
    # Assuming split_data is your dictionary with author sections as keys
    section_dictionary = {}
    for author, word_counts in split_data.items():
        filename = f"authors_csv/author_section_{iterator}.csv"
        section_dictionary[author] = word_counts
    create_authors_csv_with_counts(section_dictionary, filename)
    section_dictionary = {}
    
```

```{python}
author_dict_to_csv(authors_word_freq, "authors.csv")
```
```{python}
def remove_rows_without_letters(input_csv, output_csv):
    with open(input_csv, 'r', newline='', encoding='latin-1') as infile, \
            open(output_csv, 'w', newline='', encoding='latin-1') as outfile:
        reader = csv.DictReader(infile)
        fieldnames = reader.fieldnames
        writer = csv.DictWriter(outfile, fieldnames=fieldnames)
        writer.writeheader()

        for row in reader:
            word = row['Word']
            # Check if the word contains any letters
            if any(letter.isalpha() for letter in word):
                writer.writerow(row)

```
```{python}

import pandas as pd
def encode_csv(file):
# Load the CSV file into a DataFrame
    df = pd.read_csv(file)

    one_hot_encoded = pd.get_dummies(df['Word'])

    # Concatenate the one-hot encoded DataFrame with the original DataFrame
    df_encoded = pd.concat([df, one_hot_encoded], axis=1)

    # Drop the original 'Word' column
    df_encoded.drop('Word', axis=1, inplace=True)

    # Save the encoded DataFrame to a new CSV file
    df_encoded.to_csv(f"encoded_{file}", index=False)
```

```{python}
def tokenize_text(text):
    tokens = word_tokenize(text)
        
    # Lowercase all tokens for case-insensitive analysis
    tokens = [token.lower() for token in tokens if token.lower() not in symbols]
    
    return tokens
```

```{python}
import shutil
def create_func_dict(file_path):
    string = ""
    with open(file_path, "r", encoding="latin-1") as file:

        # Iterate over each line in the file
        for line in file:
            if "Chapter" not in line:

                # Process each line as needed
                line = line.replace("(", "( ")
                line = line.replace("\t", " ")
                line = line.replace("\n`", " ")
                line = line.replace("\xa0", " ")
                line = line.strip()
                #line_split = line.split(" ")
                string = string + line

    text_tokenized = tokenize_text(string)

    text_tokenized = filter_function_words(text_tokenized, function_words_king_james)

    text_freq = Counter(text_tokenized)

    text_freq = dict(text_freq)
    filename = file_path.split(".")
    passage_dict_to_csv(text_freq, f"{filename[0]}_func.csv")
    return f"{filename[0]}_func.csv"

def create_grouped_func_dict(file_path, word_max):
    string = ""
    with open(file_path, "r", encoding="latin-1") as file:

        # Iterate over each line in the file
        word_count = []
        word_grouping = []
        for line in file:
            if "Chapter" not in line:
                
                # Process each line as needed
                line = line.replace("(", "( ")
                line = line.replace("\t", " ")
                line = line.replace("\n`", " ")
                line = line.replace("\xa0", " ")
                line = line.strip()
                line_split = line.split(" ")
                for i in line_split:
                    word_count.append(i)
                string = string + line 
                if len(word_count) >= word_max:
                    word_grouping.append(string)
                    string = ''
                    word_count = []
        word_grouping[len(word_grouping)-1] = word_grouping[len(word_grouping)-1] + string
    word_groups_labeled = {}
    iterator = 0
    print(len(word_grouping))
    for i in word_grouping:
        iterator += 1
        text_tokenized = tokenize_text(i)

        text_tokenized = filter_function_words(text_tokenized, function_words_king_james)

        text_freq = Counter(text_tokenized)

        text_freq = dict(text_freq)

        word_groups_labeled[f"_group_{iterator}"] = text_freq

        filename = file_path.split(".")

        folder_name = f"{filename[0]}_folder"
        if not os.path.exists(folder_name):
            os.makedirs(folder_name)
        else:
            shutil.rmtree(folder_name)
            os.makedirs(folder_name) 
        # new_file_path = os.path.join(folder_name, f"{filename[0]}_func.csv")
        for key, value in word_groups_labeled.items():
            new_file_path = os.path.join(folder_name, f"{filename[0]}{key}_func.csv")
            passage_dict_to_csv(value, new_file_path)
def create_named_grouped_func_dict(file_path, author_dict, word_max=0,):
    if word_max != 0:
        string = ""
        with open(file_path, "r", encoding="latin-1") as file:
            
            # Iterate over each line in the file
            word_count = []
            word_list=[]
            word_grouping = []
            for line in file:
                if "Chapter" not in line:
                    
                    # Process each line as needed
                    line = line.replace("(", "( ")
                    line = line.replace("\t", " ")
                    line = line.replace("\n`", " ")
                    line = line.replace("\xa0", " ")
                    line = line.strip()
                    line_split = line.split(" ")
                    for i in line_split:
                        if i not in word_list:
                            word_list.append(i)
                        word_count.append(i)
                    string = string + line 
                    if len(word_count) >= word_max:
                        word_grouping.append(string)
                        string = ''
                        word_count = []
            word_grouping[len(word_grouping)-1] = word_grouping[len(word_grouping)-1] + string
            word_groups_labeled = {}
            iterator = 0
            for i in word_grouping:
                iterator += 1
                text_tokenized = tokenize_text(i)

                text_tokenized = filter_function_words(text_tokenized, function_words_king_james)

                text_freq = Counter(text_tokenized)

                text_freq = dict(text_freq)

                word_groups_labeled[f"Group_{iterator}"] = text_freq

                filename = file_path.split(".")

                name_list = author_dict[filename[0]]
                name = name_list[0]

                folder_name = f"{name}_folder"
                if not os.path.exists(folder_name):
                    os.makedirs(folder_name)
                #else:
                    #shutil.rmtree(folder_name)
                    #os.makedirs(folder_name) 
                #new_file_path = os.path.join(folder_name, f"{filename[0]}_func.csv")
                iterator = 0
                for key, value in word_groups_labeled.items():
                    iterator += 1
                    new_file_path = os.path.join(folder_name, f"{name}_group_{iterator}_name_include_func.csv")
                    named_author_dict_to_csv(name, value, new_file_path)
            #return f"{filename[0]}_func.csv"
    else:
        string = ""
        with open(file_path, "r", encoding="latin-1") as file:
            
            # Iterate over each line in the file
            word_count = []
            word_list = []
            for line in file:
                if "Chapter" not in line:
                    
                    # Process each line as needed
                    line = line.replace("(", "( ")
                    line = line.replace("\t", " ")
                    line = line.replace("\n`", " ")
                    line = line.replace("\xa0", " ")
                    line = line.strip()
                    line_split = line.split(" ")
                    for i in line_split:
                        if i not in word_list:
                            word_list.append(i)
                    string = string + line 
            text_tokenized = tokenize_text(string)

            text_tokenized = filter_function_words(text_tokenized, function_words_king_james)

            text_freq = Counter(text_tokenized)

            text_freq = dict(text_freq)

            filename = file_path.split(".")
            name_list = author_dict[filename[0]]
            name = name_list[0]
            folder_name = f"{name}_folder"
            if not os.path.exists(folder_name):
                os.makedirs(folder_name)
            new_file_path = os.path.join(folder_name, f"{name}_name_include_func.csv")
            named_author_dict_to_csv(name, text_freq, new_file_path,)
            #return f"{filename[0]}_func.csv"

```

```{python}
def create_txt_dict(file_path):
    string = ""
    # file = "nephi_chapter_2.txt"
    with open(file_path, "r", encoding="latin-1") as file:

        # Iterate over each line in the file
        for line in file:
            if "Chapter" not in line:
                # Process each line as needed
                line = line.replace("(", "( ")
                line = line.replace("\t", " ")
                line = line.replace("\n`", " ")
                line = line.replace("\xa0", " ")
                line = line.strip()
                #line_split = line.split(" ")
                string = string + line

    text_tokenized = tokenize_text(string)

    text_freq = Counter(text_tokenized)

    text_freq = dict(text_freq)
    filename = file_path.split(".")
    passage_dict_to_csv(text_freq, f"{filename[0]}.csv")
    return f"{filename[0]}.csv"

#encode_csv("nephi_1_te.csv")
    
```

```{python}
import pandas as pd
def create_author_func_aggr_csv(file_path, filename):
# Read the CSV file
    df = pd.read_csv(file_path)

    # Add a column for the total word count for each author
    df['Total_Word_Count'] = df.iloc[:, 1:].sum(axis=1)

    # Calculate the percentage of total word count for each word
    for word in df.columns[1:-1]:  # Exclude the first and last column
        df[word] = (df[word] / df['Total_Word_Count']) * 100

    sorted_columns = sorted(df.columns[1:-1])
    df = df[df.columns[:1].tolist() + sorted_columns + df.columns[-1:].tolist()]
    # Output the DataFrame to a new CSV file
    df.to_csv(filename, index=False)

def remove_non_ascii(text):
    return ''.join(char for char in text if ord(char) < 128)

def create_authors_csv_with_counts(data, filename):
    rows = []
    for author, word_counts in data.items():
        author = author.split("_")[0]
        author = remove_non_ascii(author)
        if author != "":
            row = {'Author': author}
            row.update(word_counts)
            rows.append(row)
    # Create DataFrame
    df = pd.DataFrame(rows)

    ("authors_aggregated_with_percentage.csv")
    unique_words = original_df.columns.tolist()[1:-1]  # Extract unique words from the original CSV


    # Pivot the DataFrame to get the desired format, treating 'Word' as the index
    df_pivot = df.pivot_table(index=None, columns='Word', values='Count', aggfunc='sum', fill_value=0)

    # Ensure that the new DataFrame includes all the same columns as the original CSV
    missing_columns = set(unique_words) - set(df_pivot.columns)
    for column in missing_columns:
        df[column] = 0

    # Fill NaN values with 0
    df.fillna(0, inplace=True)

    # Add a column for the total word count for each author
    df['Total_Word_Count'] = df.drop('Author', axis=1).sum(axis=1)

    # Output the DataFrame to a new CSV file
    df.to_csv(filename, index=False)

# print("Output CSV file generated successfully.")
```
```{python}
import shutil
folder_path = "authors_csv"
#shutil.rmtree(folder_path)
tokenized_text_list = tokenize_dict(BoM_verse_seperation("BoM.txt", 10))
iterator = 0
for i in tokenized_text_list:
    iterator += 1
    create_section_csv(i, 10, iterator)
for filename in os.listdir(folder_path):
        new_file_path = f"{filename.split(".")[0]}_aggr.csv"
        filename = os.path.join(folder_path, filename)
        new_file_path = os.path.join(folder_path, new_file_path)
        create_author_func_aggr_csv(filename, new_file_path)
```


```{python}
import pandas as pd
def create_aggr_func_csv(file_path):
    # Read the original CSV file to get unique words
    original_df = pd.read_csv("authors_aggregated_with_percentage.csv")
    unique_words = original_df.columns.tolist()[1:-1]  # Extract unique words from the original CSV

    # Read the new CSV file into a DataFrame
    new_df = pd.read_csv(file_path)

    # Pivot the DataFrame to get the desired format, treating 'Word' as the index
    df_pivot = new_df.pivot_table(index=None, columns='Word', values='Count', aggfunc='sum', fill_value=0)

    # Ensure that the new DataFrame includes all the same columns as the original CSV
    missing_columns = set(unique_words) - set(df_pivot.columns)
    for column in missing_columns:
        df_pivot[column] = 0  # Add missing columns filled with zeros

    # Reset the index to make the DataFrame consistent
    df_pivot.reset_index(inplace=True)

    # Reorder columns to match the original CSV (in alphabetical order)
    df_pivot = df_pivot[sorted(df_pivot.columns)]

    # Calculate the total word count
    total_word_count = df_pivot.sum(axis=1)

    # Calculate the percentage of total word count for each word
    for word in unique_words:
        df_pivot[word] = (df_pivot[word] / total_word_count) * 100

    df_pivot['Total_Word_Count'] = total_word_count

    filename = file_path.split(".")
    # Output the DataFrame to a new CSV file
    df_pivot.to_csv(f"{filename[0]}_aggr.csv", index=False)

def create_named_aggr_func_csv(file_path, author_name):
    # Read the original CSV file to get unique words
    original_df = pd.read_csv("authors_aggregated_with_percentage.csv")
    unique_words = original_df.columns.tolist()[1:-1]  # Extract unique words from the original CSV

    # Read the new CSV file into a DataFrame
    new_df = pd.read_csv(file_path)

    # Add an "Author" column with the specified author's name
    new_df['Author'] = author_name

    # Pivot the DataFrame to get the desired format, treating 'Word' as the index
    df_pivot = new_df.pivot_table(index='Author', columns='Word', values='Count', aggfunc='sum', fill_value=0)

    # Ensure that the new DataFrame includes all the same columns as the original CSV
    missing_columns = set(unique_words) - set(df_pivot.columns)
    for column in missing_columns:
        df_pivot[column] = 0  # Add missing columns filled with zeros

    # Reset the index to make the DataFrame consistent
    df_pivot.reset_index(inplace=True)

    # Reorder columns to match the original CSV (in alphabetical order)
    df_pivot = df_pivot[['Author'] + sorted(unique_words)]

    # Calculate the total word count
    total_word_count = df_pivot[unique_words].sum(axis=1)

    # Calculate the percentage of total word count for each word
    for word in unique_words:
        df_pivot[word] = (df_pivot[word] / total_word_count) * 100

    df_pivot['Total_Word_Count'] = total_word_count

    # Output the DataFrame to a new CSV file
    filename_parts = file_path.split(".")
    output_filename = f"{filename_parts[0]}_aggr.csv"
    df_pivot.to_csv(output_filename, index=False)
```

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import cross_val_score
# Step 1: Data Preprocessing
data = pd.read_csv("authors_aggregated_with_percentage.csv", encoding='latin-1')

# Splitting data into features and target variable
X_train = data.drop(columns=["Author", "Total_Word_Count"])
X_train.reset_index(drop=True, inplace=True)
y_train = data["Author"]

hyperparameters2 = {
    'n_estimators': 100,
    'max_depth': None,
    # Add more hyperparameters as needed
}

model = RandomForestClassifier(**hyperparameters2)
model.fit(X_train, y_train)

cv_scores = cross_val_score(model, X_train, y_train, cv=5)

# Print cross-validation scores
print("Cross-validation scores:", cv_scores)
print("Mean CV score:", np.mean(cv_scores))
print("Standard deviation of CV scores:", np.std(cv_scores))

# Once the model is trained and evaluated, you can use it to predict the author of a passage
# Extract function words from the passage and compute their frequencies
# Pass the computed frequencies into the trained model to predict the author
predicted_author = model.predict(new_passage_function_word_frequencies)
print("Predicted author:", predicted_author)
```
```{python}
def run_predict_group(folder_path, model):
    files = []
    guesses = {}
    for filename in os.listdir(folder_path):
        # Check if the filename contains "aggr"
        if "aggr" in filename and "name" not in filename:
            # Process the file (e.g., print its name)
            files.append(filename)
    print(files)
    for i in files:
        new_file_path = os.path.join(folder_path, i)
        new_data = pd.read_csv(new_file_path)
        new_data = new_data.drop(columns=["Total_Word_Count"])
        prediction = model.predict(new_data)
        feature_importance = model.feature_importances_
        feature_importance_dict = dict(zip(new_data.columns, feature_importance))
        sorted_feature_importance = dict(sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True))
        guesses[i] = prediction    
    return guesses, sorted_feature_importance, len(files)


```
```{python}
import pandas as pd
import numpy as np
import os

# Define function to load training data from individual files
def load_training_data(folder_path, count, authors_to_include=[]):
    numbers_list = [num for num in range(1, count+1)]
    data_list = []
    filename_list = []
    iterator = 0
    for filename in os.listdir(folder_path):
        # Check if the filename contains "aggr"
            if "aggr" in filename:
                num = filename.split("_")[2]
                if int(num) in numbers_list:
                    if iterator < count:
                        iterator += 1
                        file_path = os.path.join(folder_path, filename)
                        df = pd.read_csv(file_path, encoding='latin-1')
                        if authors_to_include:
                            df = df[df["Author"].isin(authors_to_include)]
                        # Replace NaN values with 0.0
                        df.fillna(0.0, inplace=True)
                        if df.isnull().values.any():
                            print(f"NaN values found in DataFrame: {filename}")
                            print(df[df.isnull().any(axis=1)])
                        data_list.append(df)
                        filename_list.append(filename)
    return pd.concat(data_list, ignore_index=True)

# Define function to load testing data from a single file
def load_testing_data(folder_path, count, count_adder, authors_to_include=[]):
    numbers_list = [num for num in range(count_adder+1, count_adder+count+1)]
    data_list = []
    filename_list = []
    iterator = 0
    count_iterator = 1
    for filename in os.listdir(folder_path):
        # Check if the filename contains "aggr"
            num = filename.split("_")[2]
            if "aggr" in filename and int(num) in numbers_list:
                count_iterator += 1
                if iterator < count:
                    iterator += 1
                    file_path = os.path.join(folder_path, filename)
                    df = pd.read_csv(file_path, encoding='latin-1')
                    # Replace NaN values with 0.0
                    if authors_to_include:
                        df = df[df["Author"].isin(authors_to_include)]
                    df.fillna(0.0, inplace=True)
                    data_list.append(df)
                    filename_list.append(filename)
    return pd.concat(data_list, ignore_index=True)
```

```{python}
from sklearn.utils import shuffle
# Define the folder path containing the training data files
train_folder_path = "authors_csv"
desired_authors = ["nephi", "jacob", "enos", "moroni", "mormon"]
# Load the training data
X_train = load_training_data("authors_csv", 8, desired_authors)
X_train.fillna(0.0, inplace=True)
#print(X_train)
X_train = shuffle(X_train)
# Splitting data into features and target variable
y_train = X_train["Author"]
X_train = X_train.drop(columns=["Author", "Total_Word_Count"])

#print(X_train)

hyperparameters = {
    'max_depth': None,
    # Add more hyperparameters as needed
}
# hyperparameters = {
#     'n_estimators': 100,
#     'max_depth': None,
#     'min_samples_split': 10,
#     'min_samples_leaf': 4,
#     'max_features': 'sqrt',
#     'bootstrap': True,
#     'random_state': 42
# }

# Initialize the Random Forest Classifier with the hyperparameters
model = RandomForestClassifier(**hyperparameters)

model.fit(X_train, y_train)
# Perform cross-validation with 5 folds
cv_scores = cross_val_score(model, X_train, y_train, cv=8)

# Print the cross-validation scores
print("Cross-validation scores:", cv_scores)

# Calculate and print the mean and standard deviation of the cross-validation scores
print("Mean CV score:", np.mean(cv_scores))
print("Standard deviation of CV scores:", np.std(cv_scores))

feature_importances = model.feature_importances_
importances_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})
importances_df = importances_df.sort_values(by='Importance', ascending=False)

importances_df_top5 = importances_df.head(5)
print(importances_df_top5)
```

```{python}
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


test_folder_path = "authors_csv"
desired_authors = ["nephi", "jacob", "enos", "moroni", "mormon"]
X_test = load_testing_data(test_folder_path, 2, 8, desired_authors)
X_test.fillna(0.0, inplace=True)
X_test = shuffle(X_test)
# Step 2: Preprocess testing data (drop columns not used in training)
X_test_processed = X_test.drop(columns=["Author", "Total_Word_Count"])

# Step 3: Make predictions using the trained model
predictions = model.predict(X_test_processed)
#y_true = X_test["Author"]
# Optional: Print the predictions
#print(predictions)
y_true = X_test["Author"]
#print(y_true)
print(predictions)
accuracy = accuracy_score(y_true, predictions)
precision = precision_score(y_true, predictions, average='weighted')  # Other options: 'micro', 'macro'
recall = recall_score(y_true, predictions, average='weighted')  # Other options: 'micro', 'macro'
f1 = f1_score(y_true, predictions, average='weighted')  # Other options: 'micro', 'macro'

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1)



```
```{python}
text = ""
folder_path = folder_path = f'{text}_folder'
guesses, feature_importance_dict, amount_of_files = run_predict_group(folder_path, model)
correct_name = "nephi"
total_correct_guesses = 0
incorrect_guesses = 0
for key, value in guesses.items():
    if value[0] == "nephi":
        total_correct_guesses += 1
    else:
        incorrect_guesses += 1
print(guesses)
print(f"correct: {total_correct_guesses}, incorrect: {incorrect_guesses}")
```
```{python}
create_aggr_func_csv(create_func_dict("d&c_full.txt"))
```
```{python}
author_dict = {'nephi1': ['nephi'], 'nephi2': ['nephi'], 'jacob': ['jacob'], 'enos': ['enos'], 'mosiah': ['mormon'], 'alma': ['mormon'], 'helaman': ['mormon'], 'nephi3': ['mormon'], 'nephi4': ['mormon'], 'mormon': ['mormon'], 'ether': ['moroni'], 'moroni': ['moroni']}

text = "moroni"
create_grouped_func_dict(f"{text}.txt", 3000)
create_named_grouped_func_dict(f"{text}.txt", author_dict,3000)
folder_path = f'{text}_folder'

new_file_path = os.path.join(folder_path, f"{text}_func.csv")


# Iterate through files in the folder
for filename in os.listdir(folder_path):
    # Check if the entry is a file (not a directory)
    if os.path.isfile(os.path.join(folder_path, filename)):
        new_file_path = os.path.join(folder_path, filename)
        if "name" not in new_file_path:
            create_aggr_func_csv(new_file_path)
        else:
            create_named_aggr_func_csv(new_file_path, text)


```
```{python}
text = "moroni"
folder_path = folder_path = f'{text}_folder'
guesses, feature_importance_dict, amount_of_files = run_predict_group(folder_path, model)
correct_name = "moroni"
total_correct_guesses = 0
incorrect_guesses = 0
for key, value in guesses.items():
    if value[0] == "moroni":
        total_correct_guesses += 1
    else:
        incorrect_guesses += 1
print(guesses)
print(f"correct: {total_correct_guesses}, incorrect: {incorrect_guesses}")
output_folder = f"{text}_output"
if not os.path.exists(output_folder):
    os.makedirs(output_folder)
filename = f"{text} run for {amount_of_files} files"
file_path = os.path.join(output_folder, filename)
write_to_text_file(file_path, total_correct_guesses, incorrect_guesses, feature_importance_dict, amount_of_files, text)

```
```{python}
def write_to_text_file(filename, correct_guesses, wrong_guesses, feature_importance_dict, num_data, book_name):
    with open(filename, 'w') as file:
        file.write("Number of correct guesses: {}\n".format(correct_guesses))
        file.write("Number of wrong guesses: {}\n".format(wrong_guesses))
        file.write("Feature Importance Dictionary:\n")
        for feature, importance in feature_importance_dict.items():
            file.write("{}: {}\n".format(feature, importance))
        file.write("Number of data ran for predictions: {}\n".format(num_data))
        file.write("Name of book data obtained from: {}\n".format(book_name))
```

```{python}
import matplotlib.pyplot as plt
is_make_dict = is_make_dict = {
    'whole_BoM': {"is": 0.884257775, "make": 0.112856685},
    'd&c': {"is": 1.984950916, "make": 0.101323854},
    'enos': {'is': 0.789889415, 'make': 0},
    'jacob': {'is': 0.772200772, 'make': 0.077220077},
    'mormon': {'is': 0.796048016, 'make': 0.076974043},
    'moroni': {'is': 0.985768377, 'make': 0.091395744},
    'nephi': {'is': 1.109652413, 'make': 0.234433608},
}
authors = list(is_make_dict.keys())

# Extract frequencies of "is" and "make"
is_frequency = [is_make_dict[author]["is"] for author in authors]
make_frequency = [is_make_dict[author]["make"] for author in authors]

# Plot scatter plot
plt.scatter(is_frequency, make_frequency, color='blue')

# Add labels and title
plt.xlabel('Frequency of "is"')
plt.ylabel('Frequency of "make"')
plt.title('Frequency of "is" vs "make" Used by Authors')

# Add author names as annotations
for i, author in enumerate(authors):
    plt.annotate(author, (is_frequency[i], make_frequency[i]))

# Show plot
plt.grid(True)
plt.show()
```
```{python}
the_of_dict = {
    'whole_BoM': {"the": 12.29111895, "of": 7.538954793 },
    'd&c': {"the": 12.40801953, "of": 6.815274986},
    'enos': {'the': 10.11058452, 'of': 6.63507109},
    'jacob': {'the': 13.88030888, 'of': 7.664092664},
    'mormon': {'the': 11.70979811, 'of': 7.220944735},
    'moroni': {'the': 12.37759499, 'of': 7.905731819},
    'nephi': {'the': 13.58777194, 'of': 8.161415354},
}
authors = list(the_of_dict.keys())
print(the_of_dict)
# Extract frequencies of "is" and "make"
the_frequency = [the_of_dict[author]["the"] for author in authors]
of_frequency = [the_of_dict[author]["of"] for author in authors]

# Plot scatter plot
plt.scatter(the_frequency, of_frequency, color='blue')

# Add labels and title
plt.xlabel('Frequency of "the"')
plt.ylabel('Frequency of "of"')
plt.title('Frequency of "the" vs "of" Used by Authors')

# Add author names as annotations
for i, author in enumerate(authors):
    plt.annotate(author, (the_frequency[i], of_frequency[i]))

# Show plot
plt.grid(True)
plt.show()
```
```{python}
with_had_dict = {
    'whole_BoM': {"with":0.970823982, "had":1.133055466},
    'd&c': {"with": 1.011577496, "had": 0.101323854},
    'enos': {'with': 1.105845182, 'had': 1.579778831},
    'jacob': {'with': 0.694980695, 'had': 0.888030888},
    'mormon': {'with': 1.015277886, 'had': 1.306610024},
    'moroni': {'with': 0.816033425, 'had': 0.900900901},
    'nephi': {'with': 0.915853963, 'had': 0.659539885},
}
authors = list(with_had_dict.keys())
# Extract frequencies of "is" and "make"
with_frequency = [with_had_dict[author]["with"] for author in authors]
had_frequency = [with_had_dict[author]["had"] for author in authors]

# Plot scatter plot
plt.scatter(with_frequency, had_frequency, color='blue')

# Add labels and title
plt.xlabel('Frequency of "with"')
plt.ylabel('Frequency of "had"')
plt.title('Frequency of "with" vs "had" Used by Authors')

# Add author names as annotations
for i, author in enumerate(authors):
    plt.annotate(author, (with_frequency[i], had_frequency[i]))

# Show plot
plt.grid(True)
plt.show()
```